{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset Loading and Transformation\n",
        "# Data Augmentation and Normalization for Training, Normalization for Testing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the images on the horizontal\n",
        "    transforms.RandomRotation(10),      # Random rotation of the images by +/- 10 degrees\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 256  # Example batch size, can be tuned\n",
        "learning_rate = 0.001  # Example learning rate, can be tuned\n",
        "num_epochs = 105  # Example number of epochs, can be tuned\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class IntermediateBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_layers, num_channels, dropout_prob=0.5):\n",
        "        super(IntermediateBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(num_layers):\n",
        "            self.layers.append(nn.Sequential(\n",
        "                nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(num_channels),\n",
        "                nn.Dropout(dropout_prob),\n",
        "                nn.ReLU()\n",
        "            ))\n",
        "        self.fc = nn.Linear(in_channels, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        m = x.mean([2, 3])  # Global average pooling\n",
        "        a = torch.sigmoid(self.fc(m))  # Compute a vector\n",
        "        outputs = [layer(x) for layer in self.layers]\n",
        "        a = a.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # Reshape a to [batch_size, num_layers, 1, 1, 1]\n",
        "        outputs = torch.stack(outputs, dim=1)  # Shape [batch_size, num_layers, num_channels, H, W]\n",
        "        x_prime = (a * outputs).sum(dim=1)  # Sum across the layer dimension\n",
        "        return x_prime\n",
        "\n",
        "\n",
        "class BasicArch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicArch, self).__init__()\n",
        "        self.block1 = IntermediateBlock(3, 3, 64)\n",
        "        self.block2 = IntermediateBlock(64, 3, 128)\n",
        "        self.block3 = IntermediateBlock(128, 3, 256)\n",
        "        self.block4 = IntermediateBlock(256, 3, 256)\n",
        "        self.fc = nn.Linear(256, 10)  # Final layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.mean([2, 3])  # Global average pooling before the final layer\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "model = BasicArch().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(model, train_loader, test_loader, num_epochs, optimizer, criterion):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "        test_accuracy = test()\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return train_losses, train_accuracies, test_accuracies\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct_test / total_test\n",
        "    return test_acc\n",
        "\n",
        "train(model, train_loader, test_loader, num_epochs, optimizer, criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGSE4tg5Uklt",
        "outputId": "2321a080-2bb1-4957-93d9-ae3ed9ca2a3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/105], Loss: 1.5869, Training Accuracy: 41.27%, Test Accuracy: 47.46%\n",
            "Epoch [2/105], Loss: 1.2657, Training Accuracy: 54.52%, Test Accuracy: 51.85%\n",
            "Epoch [3/105], Loss: 1.1452, Training Accuracy: 59.07%, Test Accuracy: 54.76%\n",
            "Epoch [4/105], Loss: 1.0619, Training Accuracy: 62.13%, Test Accuracy: 61.47%\n",
            "Epoch [5/105], Loss: 0.9927, Training Accuracy: 64.97%, Test Accuracy: 55.38%\n",
            "Epoch [6/105], Loss: 0.9399, Training Accuracy: 66.75%, Test Accuracy: 61.24%\n",
            "Epoch [7/105], Loss: 0.8961, Training Accuracy: 68.39%, Test Accuracy: 59.13%\n",
            "Epoch [8/105], Loss: 0.8654, Training Accuracy: 69.64%, Test Accuracy: 48.40%\n",
            "Epoch [9/105], Loss: 0.8345, Training Accuracy: 70.44%, Test Accuracy: 59.04%\n",
            "Epoch [10/105], Loss: 0.8038, Training Accuracy: 71.73%, Test Accuracy: 57.06%\n",
            "Epoch [11/105], Loss: 0.7736, Training Accuracy: 72.98%, Test Accuracy: 59.53%\n",
            "Epoch [12/105], Loss: 0.7546, Training Accuracy: 73.61%, Test Accuracy: 51.54%\n",
            "Epoch [13/105], Loss: 0.7320, Training Accuracy: 74.25%, Test Accuracy: 68.27%\n",
            "Epoch [14/105], Loss: 0.7129, Training Accuracy: 75.02%, Test Accuracy: 65.14%\n",
            "Epoch [15/105], Loss: 0.6995, Training Accuracy: 75.69%, Test Accuracy: 75.52%\n",
            "Epoch [16/105], Loss: 0.6818, Training Accuracy: 76.03%, Test Accuracy: 70.85%\n",
            "Epoch [17/105], Loss: 0.6661, Training Accuracy: 76.59%, Test Accuracy: 70.90%\n",
            "Epoch [18/105], Loss: 0.6522, Training Accuracy: 77.06%, Test Accuracy: 69.98%\n",
            "Epoch [19/105], Loss: 0.6385, Training Accuracy: 77.70%, Test Accuracy: 71.90%\n",
            "Epoch [20/105], Loss: 0.6266, Training Accuracy: 77.95%, Test Accuracy: 73.95%\n",
            "Epoch [21/105], Loss: 0.6119, Training Accuracy: 78.63%, Test Accuracy: 71.74%\n",
            "Epoch [22/105], Loss: 0.6035, Training Accuracy: 78.87%, Test Accuracy: 72.20%\n",
            "Epoch [23/105], Loss: 0.5934, Training Accuracy: 79.20%, Test Accuracy: 77.19%\n",
            "Epoch [24/105], Loss: 0.5828, Training Accuracy: 79.64%, Test Accuracy: 74.27%\n",
            "Epoch [25/105], Loss: 0.5724, Training Accuracy: 80.05%, Test Accuracy: 73.97%\n",
            "Epoch [26/105], Loss: 0.5634, Training Accuracy: 80.46%, Test Accuracy: 74.27%\n",
            "Epoch [27/105], Loss: 0.5572, Training Accuracy: 80.58%, Test Accuracy: 79.00%\n",
            "Epoch [28/105], Loss: 0.5456, Training Accuracy: 80.92%, Test Accuracy: 76.23%\n",
            "Epoch [29/105], Loss: 0.5401, Training Accuracy: 81.02%, Test Accuracy: 76.20%\n",
            "Epoch [30/105], Loss: 0.5285, Training Accuracy: 81.50%, Test Accuracy: 79.49%\n",
            "Epoch [31/105], Loss: 0.5222, Training Accuracy: 81.57%, Test Accuracy: 75.31%\n",
            "Epoch [32/105], Loss: 0.5129, Training Accuracy: 81.77%, Test Accuracy: 78.60%\n",
            "Epoch [33/105], Loss: 0.5057, Training Accuracy: 82.19%, Test Accuracy: 81.18%\n",
            "Epoch [34/105], Loss: 0.4989, Training Accuracy: 82.41%, Test Accuracy: 76.71%\n",
            "Epoch [35/105], Loss: 0.4893, Training Accuracy: 82.80%, Test Accuracy: 76.99%\n",
            "Epoch [36/105], Loss: 0.4876, Training Accuracy: 82.79%, Test Accuracy: 79.27%\n",
            "Epoch [37/105], Loss: 0.4774, Training Accuracy: 83.09%, Test Accuracy: 76.72%\n",
            "Epoch [38/105], Loss: 0.4707, Training Accuracy: 83.50%, Test Accuracy: 79.16%\n",
            "Epoch [39/105], Loss: 0.4633, Training Accuracy: 83.63%, Test Accuracy: 78.02%\n",
            "Epoch [40/105], Loss: 0.4581, Training Accuracy: 84.05%, Test Accuracy: 75.17%\n",
            "Epoch [41/105], Loss: 0.4520, Training Accuracy: 84.15%, Test Accuracy: 79.87%\n",
            "Epoch [42/105], Loss: 0.4513, Training Accuracy: 84.28%, Test Accuracy: 81.33%\n",
            "Epoch [43/105], Loss: 0.4409, Training Accuracy: 84.62%, Test Accuracy: 79.05%\n",
            "Epoch [44/105], Loss: 0.4382, Training Accuracy: 84.66%, Test Accuracy: 81.59%\n",
            "Epoch [45/105], Loss: 0.4317, Training Accuracy: 84.78%, Test Accuracy: 82.48%\n",
            "Epoch [46/105], Loss: 0.4229, Training Accuracy: 85.15%, Test Accuracy: 80.53%\n",
            "Epoch [47/105], Loss: 0.4205, Training Accuracy: 85.32%, Test Accuracy: 78.85%\n",
            "Epoch [48/105], Loss: 0.4136, Training Accuracy: 85.49%, Test Accuracy: 82.18%\n",
            "Epoch [49/105], Loss: 0.4097, Training Accuracy: 85.66%, Test Accuracy: 81.45%\n",
            "Epoch [50/105], Loss: 0.4045, Training Accuracy: 85.67%, Test Accuracy: 82.67%\n",
            "Epoch [51/105], Loss: 0.4037, Training Accuracy: 85.96%, Test Accuracy: 82.81%\n",
            "Epoch [52/105], Loss: 0.3958, Training Accuracy: 86.12%, Test Accuracy: 82.00%\n",
            "Epoch [53/105], Loss: 0.3948, Training Accuracy: 86.10%, Test Accuracy: 80.31%\n",
            "Epoch [54/105], Loss: 0.3892, Training Accuracy: 86.44%, Test Accuracy: 82.48%\n",
            "Epoch [55/105], Loss: 0.3881, Training Accuracy: 86.59%, Test Accuracy: 81.94%\n",
            "Epoch [56/105], Loss: 0.3751, Training Accuracy: 86.81%, Test Accuracy: 80.02%\n",
            "Epoch [57/105], Loss: 0.3777, Training Accuracy: 86.60%, Test Accuracy: 81.65%\n",
            "Epoch [58/105], Loss: 0.3751, Training Accuracy: 86.88%, Test Accuracy: 83.16%\n",
            "Epoch [59/105], Loss: 0.3639, Training Accuracy: 87.07%, Test Accuracy: 82.64%\n",
            "Epoch [60/105], Loss: 0.3597, Training Accuracy: 87.46%, Test Accuracy: 79.76%\n",
            "Epoch [61/105], Loss: 0.3580, Training Accuracy: 87.22%, Test Accuracy: 80.77%\n",
            "Epoch [62/105], Loss: 0.3533, Training Accuracy: 87.35%, Test Accuracy: 83.57%\n",
            "Epoch [63/105], Loss: 0.3485, Training Accuracy: 87.77%, Test Accuracy: 82.21%\n",
            "Epoch [64/105], Loss: 0.3476, Training Accuracy: 87.67%, Test Accuracy: 82.32%\n",
            "Epoch [65/105], Loss: 0.3411, Training Accuracy: 87.79%, Test Accuracy: 82.79%\n",
            "Epoch [66/105], Loss: 0.3366, Training Accuracy: 88.22%, Test Accuracy: 83.00%\n",
            "Epoch [67/105], Loss: 0.3296, Training Accuracy: 88.32%, Test Accuracy: 83.46%\n",
            "Epoch [68/105], Loss: 0.3292, Training Accuracy: 88.35%, Test Accuracy: 80.85%\n",
            "Epoch [69/105], Loss: 0.3326, Training Accuracy: 88.11%, Test Accuracy: 81.54%\n",
            "Epoch [70/105], Loss: 0.3251, Training Accuracy: 88.54%, Test Accuracy: 82.87%\n",
            "Epoch [71/105], Loss: 0.3234, Training Accuracy: 88.61%, Test Accuracy: 80.34%\n",
            "Epoch [72/105], Loss: 0.3181, Training Accuracy: 88.77%, Test Accuracy: 83.82%\n",
            "Epoch [73/105], Loss: 0.3194, Training Accuracy: 88.67%, Test Accuracy: 83.10%\n",
            "Epoch [74/105], Loss: 0.3092, Training Accuracy: 89.01%, Test Accuracy: 79.30%\n",
            "Epoch [75/105], Loss: 0.3082, Training Accuracy: 89.08%, Test Accuracy: 80.99%\n",
            "Epoch [76/105], Loss: 0.3120, Training Accuracy: 88.96%, Test Accuracy: 84.44%\n",
            "Epoch [77/105], Loss: 0.2981, Training Accuracy: 89.44%, Test Accuracy: 82.51%\n",
            "Epoch [78/105], Loss: 0.2980, Training Accuracy: 89.39%, Test Accuracy: 83.05%\n",
            "Epoch [79/105], Loss: 0.2962, Training Accuracy: 89.45%, Test Accuracy: 83.04%\n",
            "Epoch [80/105], Loss: 0.2944, Training Accuracy: 89.52%, Test Accuracy: 84.29%\n",
            "Epoch [81/105], Loss: 0.2945, Training Accuracy: 89.61%, Test Accuracy: 84.00%\n",
            "Epoch [82/105], Loss: 0.2794, Training Accuracy: 90.04%, Test Accuracy: 84.07%\n",
            "Epoch [83/105], Loss: 0.2852, Training Accuracy: 89.87%, Test Accuracy: 84.29%\n",
            "Epoch [84/105], Loss: 0.2791, Training Accuracy: 90.06%, Test Accuracy: 84.94%\n",
            "Epoch [85/105], Loss: 0.2782, Training Accuracy: 90.00%, Test Accuracy: 84.39%\n",
            "Epoch [86/105], Loss: 0.2777, Training Accuracy: 90.13%, Test Accuracy: 84.14%\n",
            "Epoch [87/105], Loss: 0.2711, Training Accuracy: 90.35%, Test Accuracy: 84.57%\n",
            "Epoch [88/105], Loss: 0.2740, Training Accuracy: 90.24%, Test Accuracy: 84.52%\n",
            "Epoch [89/105], Loss: 0.2708, Training Accuracy: 90.24%, Test Accuracy: 84.32%\n",
            "Epoch [90/105], Loss: 0.2696, Training Accuracy: 90.33%, Test Accuracy: 82.74%\n",
            "Epoch [91/105], Loss: 0.2623, Training Accuracy: 90.63%, Test Accuracy: 84.80%\n",
            "Epoch [92/105], Loss: 0.2612, Training Accuracy: 90.78%, Test Accuracy: 84.73%\n",
            "Epoch [93/105], Loss: 0.2616, Training Accuracy: 90.70%, Test Accuracy: 84.80%\n",
            "Epoch [94/105], Loss: 0.2588, Training Accuracy: 90.72%, Test Accuracy: 81.61%\n",
            "Epoch [95/105], Loss: 0.2557, Training Accuracy: 90.89%, Test Accuracy: 83.71%\n",
            "Epoch [96/105], Loss: 0.2470, Training Accuracy: 91.19%, Test Accuracy: 84.77%\n",
            "Epoch [97/105], Loss: 0.2480, Training Accuracy: 91.18%, Test Accuracy: 83.04%\n",
            "Epoch [98/105], Loss: 0.2476, Training Accuracy: 91.08%, Test Accuracy: 83.45%\n",
            "Epoch [99/105], Loss: 0.2404, Training Accuracy: 91.43%, Test Accuracy: 84.43%\n",
            "Epoch [100/105], Loss: 0.2435, Training Accuracy: 91.38%, Test Accuracy: 84.59%\n",
            "Epoch [101/105], Loss: 0.2390, Training Accuracy: 91.46%, Test Accuracy: 84.19%\n",
            "Epoch [102/105], Loss: 0.2370, Training Accuracy: 91.57%, Test Accuracy: 84.41%\n",
            "Epoch [103/105], Loss: 0.2364, Training Accuracy: 91.50%, Test Accuracy: 85.48%\n",
            "Epoch [104/105], Loss: 0.2358, Training Accuracy: 91.40%, Test Accuracy: 85.45%\n",
            "Epoch [105/105], Loss: 0.2356, Training Accuracy: 91.54%, Test Accuracy: 83.61%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1.586879220199585,\n",
              "  1.2656749743652345,\n",
              "  1.1451724950790405,\n",
              "  1.0619453915023804,\n",
              "  0.9926983252716064,\n",
              "  0.9399130902099609,\n",
              "  0.8961188280868531,\n",
              "  0.8653735596466064,\n",
              "  0.8345277185058594,\n",
              "  0.8038349489593506,\n",
              "  0.7735565811920166,\n",
              "  0.7546350347518921,\n",
              "  0.732029234008789,\n",
              "  0.7129351515388489,\n",
              "  0.6994860038375854,\n",
              "  0.681779351978302,\n",
              "  0.6661499557685852,\n",
              "  0.6521569897079468,\n",
              "  0.638541390991211,\n",
              "  0.6265782439613342,\n",
              "  0.611850696105957,\n",
              "  0.603482518119812,\n",
              "  0.5934302132606506,\n",
              "  0.5828479667854309,\n",
              "  0.5724204182243348,\n",
              "  0.5634015159988404,\n",
              "  0.5572007258987427,\n",
              "  0.5456436315917969,\n",
              "  0.5400920338058471,\n",
              "  0.5285150450134277,\n",
              "  0.5221635419464111,\n",
              "  0.5128871480941772,\n",
              "  0.5056756968307495,\n",
              "  0.49885956563949585,\n",
              "  0.4893455916404724,\n",
              "  0.48758629596710207,\n",
              "  0.4774433709049225,\n",
              "  0.4707290411376953,\n",
              "  0.4633014723110199,\n",
              "  0.4580940888595581,\n",
              "  0.45201127361297605,\n",
              "  0.45133315628051757,\n",
              "  0.4409128058719635,\n",
              "  0.43816575159072874,\n",
              "  0.43174622934341433,\n",
              "  0.4228726517677307,\n",
              "  0.42046645170211794,\n",
              "  0.4136281858921051,\n",
              "  0.40968462314605714,\n",
              "  0.4044739137840271,\n",
              "  0.403737475528717,\n",
              "  0.3957787940597534,\n",
              "  0.39478149850845334,\n",
              "  0.3892179252719879,\n",
              "  0.38811268182754516,\n",
              "  0.375103759098053,\n",
              "  0.3777009211158752,\n",
              "  0.3751431304073334,\n",
              "  0.36391805583953857,\n",
              "  0.35967951860427855,\n",
              "  0.35797843368530274,\n",
              "  0.35325144055366514,\n",
              "  0.3485210008811951,\n",
              "  0.347607583193779,\n",
              "  0.34109369329452516,\n",
              "  0.3365878772163391,\n",
              "  0.32956171028137204,\n",
              "  0.32921346954345704,\n",
              "  0.332611033744812,\n",
              "  0.3251089525032043,\n",
              "  0.3233942856788635,\n",
              "  0.31806646923065185,\n",
              "  0.31942026478767394,\n",
              "  0.3092015129375458,\n",
              "  0.30819471107006075,\n",
              "  0.31196480875968935,\n",
              "  0.2981427604675293,\n",
              "  0.2980437862491608,\n",
              "  0.2961508453083038,\n",
              "  0.29439108500480654,\n",
              "  0.29451493171691895,\n",
              "  0.27937290748596194,\n",
              "  0.2852273981571197,\n",
              "  0.2790514309024811,\n",
              "  0.2782339686059952,\n",
              "  0.27769371091842654,\n",
              "  0.27113680240631105,\n",
              "  0.2739930273628235,\n",
              "  0.27083917607307434,\n",
              "  0.2696116718673706,\n",
              "  0.2623368360710144,\n",
              "  0.26119600638866425,\n",
              "  0.2616054706764221,\n",
              "  0.25879574756622314,\n",
              "  0.2556522074890137,\n",
              "  0.24700027731895446,\n",
              "  0.24796832997322082,\n",
              "  0.24760642864227295,\n",
              "  0.24035032415390015,\n",
              "  0.24348770492553712,\n",
              "  0.23904170038223266,\n",
              "  0.2370041324520111,\n",
              "  0.23637654371261596,\n",
              "  0.2357961842250824,\n",
              "  0.23562539212226868],\n",
              " [41.266,\n",
              "  54.516,\n",
              "  59.068,\n",
              "  62.126,\n",
              "  64.968,\n",
              "  66.752,\n",
              "  68.39,\n",
              "  69.638,\n",
              "  70.436,\n",
              "  71.726,\n",
              "  72.978,\n",
              "  73.61,\n",
              "  74.25,\n",
              "  75.024,\n",
              "  75.694,\n",
              "  76.028,\n",
              "  76.586,\n",
              "  77.062,\n",
              "  77.698,\n",
              "  77.952,\n",
              "  78.63,\n",
              "  78.866,\n",
              "  79.196,\n",
              "  79.642,\n",
              "  80.052,\n",
              "  80.464,\n",
              "  80.576,\n",
              "  80.916,\n",
              "  81.016,\n",
              "  81.496,\n",
              "  81.568,\n",
              "  81.768,\n",
              "  82.194,\n",
              "  82.41,\n",
              "  82.804,\n",
              "  82.786,\n",
              "  83.086,\n",
              "  83.504,\n",
              "  83.634,\n",
              "  84.054,\n",
              "  84.15,\n",
              "  84.278,\n",
              "  84.62,\n",
              "  84.656,\n",
              "  84.782,\n",
              "  85.154,\n",
              "  85.316,\n",
              "  85.49,\n",
              "  85.66,\n",
              "  85.668,\n",
              "  85.958,\n",
              "  86.124,\n",
              "  86.098,\n",
              "  86.44,\n",
              "  86.59,\n",
              "  86.808,\n",
              "  86.602,\n",
              "  86.876,\n",
              "  87.074,\n",
              "  87.462,\n",
              "  87.22,\n",
              "  87.348,\n",
              "  87.77,\n",
              "  87.668,\n",
              "  87.786,\n",
              "  88.224,\n",
              "  88.318,\n",
              "  88.35,\n",
              "  88.112,\n",
              "  88.544,\n",
              "  88.606,\n",
              "  88.774,\n",
              "  88.668,\n",
              "  89.008,\n",
              "  89.076,\n",
              "  88.96,\n",
              "  89.438,\n",
              "  89.392,\n",
              "  89.45,\n",
              "  89.516,\n",
              "  89.606,\n",
              "  90.044,\n",
              "  89.87,\n",
              "  90.058,\n",
              "  90.0,\n",
              "  90.132,\n",
              "  90.348,\n",
              "  90.236,\n",
              "  90.236,\n",
              "  90.332,\n",
              "  90.634,\n",
              "  90.784,\n",
              "  90.702,\n",
              "  90.718,\n",
              "  90.886,\n",
              "  91.192,\n",
              "  91.18,\n",
              "  91.076,\n",
              "  91.432,\n",
              "  91.378,\n",
              "  91.456,\n",
              "  91.572,\n",
              "  91.504,\n",
              "  91.398,\n",
              "  91.536],\n",
              " [47.46,\n",
              "  51.85,\n",
              "  54.76,\n",
              "  61.47,\n",
              "  55.38,\n",
              "  61.24,\n",
              "  59.13,\n",
              "  48.4,\n",
              "  59.04,\n",
              "  57.06,\n",
              "  59.53,\n",
              "  51.54,\n",
              "  68.27,\n",
              "  65.14,\n",
              "  75.52,\n",
              "  70.85,\n",
              "  70.9,\n",
              "  69.98,\n",
              "  71.9,\n",
              "  73.95,\n",
              "  71.74,\n",
              "  72.2,\n",
              "  77.19,\n",
              "  74.27,\n",
              "  73.97,\n",
              "  74.27,\n",
              "  79.0,\n",
              "  76.23,\n",
              "  76.2,\n",
              "  79.49,\n",
              "  75.31,\n",
              "  78.6,\n",
              "  81.18,\n",
              "  76.71,\n",
              "  76.99,\n",
              "  79.27,\n",
              "  76.72,\n",
              "  79.16,\n",
              "  78.02,\n",
              "  75.17,\n",
              "  79.87,\n",
              "  81.33,\n",
              "  79.05,\n",
              "  81.59,\n",
              "  82.48,\n",
              "  80.53,\n",
              "  78.85,\n",
              "  82.18,\n",
              "  81.45,\n",
              "  82.67,\n",
              "  82.81,\n",
              "  82.0,\n",
              "  80.31,\n",
              "  82.48,\n",
              "  81.94,\n",
              "  80.02,\n",
              "  81.65,\n",
              "  83.16,\n",
              "  82.64,\n",
              "  79.76,\n",
              "  80.77,\n",
              "  83.57,\n",
              "  82.21,\n",
              "  82.32,\n",
              "  82.79,\n",
              "  83.0,\n",
              "  83.46,\n",
              "  80.85,\n",
              "  81.54,\n",
              "  82.87,\n",
              "  80.34,\n",
              "  83.82,\n",
              "  83.1,\n",
              "  79.3,\n",
              "  80.99,\n",
              "  84.44,\n",
              "  82.51,\n",
              "  83.05,\n",
              "  83.04,\n",
              "  84.29,\n",
              "  84.0,\n",
              "  84.07,\n",
              "  84.29,\n",
              "  84.94,\n",
              "  84.39,\n",
              "  84.14,\n",
              "  84.57,\n",
              "  84.52,\n",
              "  84.32,\n",
              "  82.74,\n",
              "  84.8,\n",
              "  84.73,\n",
              "  84.8,\n",
              "  81.61,\n",
              "  83.71,\n",
              "  84.77,\n",
              "  83.04,\n",
              "  83.45,\n",
              "  84.43,\n",
              "  84.59,\n",
              "  84.19,\n",
              "  84.41,\n",
              "  85.48,\n",
              "  85.45,\n",
              "  83.61])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, 'ro-', label='Training Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, 'bo-', label='Training Accuracy')\n",
        "plt.plot(test_accuracies, 'go-', label='Test Accuracy')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# plot_metrics(train_losses, train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "id": "xRsJxcXAWNlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "mepvYCfB4E1c"
      }
    }
  ]
}